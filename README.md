# **Medical-Text-Classification-2**

Text Classification for Medical Question

## **Dataset**

> **Dataset from Toyhom Chinese-medical-dialogue-data** \
> repo: [https://github.com/Toyhom/Chinese-medical-dialogue-data](<https://github.com/Toyhom/Chinese-medical-dialogue-data>)

## **Usage**

train:

```bash
python run.py
```

# **Contents**

1. [Introduce](#introduce)
2. [Problem](#problem)
3. [Feature Selction](#feature-selection)
4. [Classification](#classification)
5. [Result](#result)
6. [Discussion](#discussion)
7. [Reference](#reference)

# **Introduce**

æœŸæœ«ç‚ºæœŸä¸­çš„å»¶ä¼¸ï¼Œåœ¨æœŸä¸­å ±å‘Šï¼Œç•¶æ™‚æ˜¯ä»¥åˆ†è©å¾Œåˆ†é¡çš„æ–¹å¼è©¦åœ–åšå‡ºæ–‡å­—åˆ†é¡ï¼Œåˆ¤æ–·è¼¸å…¥å•å¥å±¬æ–¼å“ªä¸€ç¨®ç§‘åˆ¥

``` mermaid
graph LR
raw((Raw Ask Text)) -->seg[Word Segment] 
seg --> rm[Remove Stopwords]
rm --> w2v[Word Embedding<br/><i>Word2Vector</i>]
subgraph Sklearn pipeline
w2v --> cls[Classifier<br/><i>Xgboost</i>]
end
cls --> out((Output Class))
```

ä½†åœ¨çµæœä¸Šä¸¦ä¸ç†æƒ³ï¼Œå°±ç®—åƒ…æœ‰å…§ç§‘èˆ‡å¤–ç§‘çš„åˆ†é¡æº–ç¢ºç‡åƒ…æœ‰å°‡è¿‘80%ã€‚æœŸæœ«æƒ³æ²¿ç”¨å…¶ä¸­çš„é¡Œç›®ï¼Œä¸¦æ‰¾æ–¹æ³•è§£æ±ºæœŸä¸­æ‰€é‡åˆ°çš„å„ç¨®å•é¡Œã€‚

ä»¥ä¸‹ç‚ºæœ¬æ¬¡çš„ pipeline

``` mermaid
graph LR
raw((Raw Ask Text)) -->seg[Word Segment] 
seg --> rm[Remove Stopwords]
rm --> w2v[Word Embedding<br/><i>Word2Vector</i>]
rm --> fs[Feature Selection<br/><i>Information gain</i>]
w2v --> cls[Classifier<br/><i>TextCNN</i>]
fs --> cls
subgraph PyTorch model
cls
end
cls --> out((Output Class))
```

# **Problem**

æœŸä¸­æ‰€ä½¿ç”¨çš„åˆ†é¡æ¨¡å‹æ˜¯Xgbooståˆ†é¡æ¨¹ï¼Œç™¼ç¾äº†ä»¥ä¸‹å•é¡Œ

## **çµæœä¸æ˜ç¢º**

å› ç‚ºæ˜¯ç”¨å­—è©ä¸‹å»åˆ†é¡ï¼Œåˆ†é¡æ¨¹åªèƒ½å°‡å­—è©æ’åˆ°æŸä¸€å€‹åˆ†é¡ï¼Œé‚£è¼¸å‡ºçµæœå°±æœƒè®Šå¾—ä¸æ˜ç¢º

```python
x= 'æˆ‘ä»Šå¤©æµ‹é«˜è¡€å‹ï¼Œèƒ¸é—·åˆèƒ¸ç—›è¦æ€ä¹ˆç¼“è§£'
xgb_pipeline.predict(ws.word_segment(x, STOP_WORDS_PATH))

>>> array([ 4,  9,  6, 13, 16], dtype=int64)
```

é€™æ˜¯ä¸Šä¸€æ¬¡çš„è¼¸å‡ºçµæœï¼ŒçœŸçœ‹ä¸å‡ºå€‹æ‰€ä»¥ç„¶ ğŸ¤”

## **Underfitting**

Xgboost çš„æ¨è–¦æ·±åº¦ç‚º 6~10ï¼Œä½†æœŸä¸­ä½¿ç”¨çš„åƒæ•¸å·²æ˜¯ 12ï¼Œè¨±å¤šé˜²æ­¢ overfitting çš„åƒæ•¸ä¹Ÿæœ‰èª¿ä½ï¼Œä½†æœ€å¾Œ training dataset è·Ÿ testing dataset çš„æº–ç¢ºç‡ä¸€ç›´æ²’æœ‰èµ·è‰²ï¼Œåˆ¤æ–·æ˜¯ underfitting äº†ã€‚

ã€Œæœ‰å¯èƒ½ã€æ˜¯ Xgboost ç„¡æ³•å­¸ç¿’å‡º 100 ä»¥ä¸Š dimension çš„ç‰¹å¾µï¼Œæ—¢ç„¶é€™æ¨£åªå¥½æ›å€‹åˆ†é¡æ¨¡å‹â”€TextCNN

# **Feature Selection**

èªæ–™åº«ä¸­æœ‰å¾ˆå¤šè©ï¼Œé›–ç„¶æˆ‘æœ‰å…ˆåˆªé™¤åœç”¨è© (stopwords)ï¼Œä½†å°æ–¼åˆ†é¡ä¾†èªªé‚„æ˜¯å¤ªå¤šè©ï¼Œç¶“ Word2Vector æ¨¡å‹å¯ä»¥å¾—çŸ¥ç¸½å…±æœ‰ 30000 ä»¥ä¸Šã€‚åœ¨é€™æ¨£çš„æƒ…æ³ä¸‹æœ‰å…©ç¨®é¸æ“‡ã€‚

- å»é™¤æ›´å¤šçš„è©
- ç‚ºæ¯å€‹è©åŠ ä¸Šæ¬Šé‡

é€™é‚Šé¸æ“‡äº†ç¬¬äºŒç¨®æ–¹æ³•ã€Œç‰¹å¾µé¸æ“‡ã€(Feature Selection)

åœ¨æœŸä¸­ä¸ç”¨åšæ˜¯å› ç‚º Xgboost æœ¬ä¾†å°±å…·æœ‰ feature selection çš„ç‰¹æ€§ã€‚

è€Œ Feature Selection åˆåˆ†ç‚ºå¾ˆå¤šç¨®ï¼Œæ‡‰ç”¨æ–¼æ–‡å­—çš„è³‡æ–™ä¸Šå¸¸è¦‹çš„æœ‰é€™å¹¾ç¨®

- Document frequency
- Mutual information
- Expected cross entropy
- Odds ratio
- Chi-square statistic
- Information gain

é¸ç”¨ Information gain

## **Information gain**

![IG](https://i.stack.imgur.com/ooDtt.png)

Information gain çš„è¨ˆç®—æ–¹æ³•

![IGtext](https://i.stack.imgur.com/Al4bp.png)

ä»¥ä¸‹å‰10é‡è¦çš„å­—è©

information_gain|word
---|---
0.0288222765244487|åŒ…çš®
0.0203681765260794|è‚›é—¨
0.0194295301512605|æ‰‹æœ¯
0.0178957357896692|ç–æ°”
0.0169325488272825|ç—”ç–®
0.0166345430373632|ä¹³è…º
0.0149232388826061|è¿‡é•¿
0.0143466830058858|ä¹³æˆ¿
0.0123994787547603|ç™«ç—«
0.0121461987535613|å’³å—½

## **Weight**

å°‡ information gain åš 0-1 æ­£è¦åŒ–æˆç‚ºæ¬Šé‡ä½¿ç”¨

ä¸éåœ¨ä¸‹é¢é€™ç¯‡ä¸­æåˆ°ï¼Œé›–ç„¶ information gain æ˜¯æœ€æœ‰æ•ˆç‡çš„ç‰¹å¾µæ“·å–æ¼”ç®—æ³•ä¹‹ä¸€ï¼Œä½†æœ‰å¯èƒ½æœƒå‡ºç¾é›–ç„¶ information gain å¾ˆä½ï¼Œä½†å…¶å¯¦ä»–å®ƒå¾ˆé‡è¦çš„æƒ…æ³ã€‚

> **Improved information gain feature selection method for Chinese text classification based on word embedding**\
> Lei Zhu1, Guijun Wang1 and Xianchun Zou\
> DOI: [https://doi.org/10.1145/3056662.3056671](https://dl.acm.org/doi/abs/10.1145/3056662.3056671)

ä»–æå‡ºäº†ä¸€å€‹æ–¹æ³•æ˜¯åˆ©ç”¨ word space ä¸­å¯ä»¥è¨ˆç®—è©å‘é‡è·Ÿè©å‘é‡ä¹‹é–“ç›¸ä¼¼åº¦çš„æ€§è³ªï¼Œå°‡æœ‰å¯èƒ½ä¹Ÿå¾ˆé‡è¦ä½† information gain éä½çš„è©å¾€ä¸ŠåŠ ï¼Œä»¥ä¸‹æ˜¯ä»–è¨ˆç®—æ¬Šé‡çš„å…¬å¼ã€‚

![weight01](https://cdn.discordapp.com/attachments/747728438814703616/988419415114666054/unknown.png)

|||
---|---
w(t, d)|word t weight in document d
tf(t, d)|frequency of word t appears in document d
N|total number of documents
n_t|number of documents in which the word t appears
phi(d)|class that document d belongs to
P(phi(d) \| t)|the probability that the word t appears in the class phi(d)
1 + P(phi(d) \| t)|to enhance the ability of the vector to distinguish text class

ä¸é....

ç›®å‰æˆ‘çš„èƒ½åŠ›é‚„ä¸è¶³ä»¥åœ¨é€™äº›æ™‚é–“ä¸‹å®Œæˆã€‚

æˆ‘çš„æƒ³æ³•æ˜¯ï¼Œæ—¢ç„¶æˆ‘æœ‰ information gainï¼Œè€Œä¸” word space æ˜¯ç”¨ Word2Vecï¼Œæœ‰ä¸€å€‹ most_similar() æ–¹æ³•ï¼Œä½¿ç”¨å‚³å›çš„ list åšæ›´å‹•å°±å¥½ã€‚

å¾information gain æœ€é«˜çš„é–‹å§‹ï¼Œæ¯å€‹è·Ÿå…¶å‰åç›¸é—œçš„å­—è©ï¼Œè‡ªå·±çš„ information gain åŠ ä¸Šè·Ÿå®ƒä¹‹é–“information gain çš„å·®ç•°ä¹˜ä»¥ similarity

|word|information gain|
---|---
ç—”ç–®|0.016932548827282567

Word space ä¸­è·Ÿç—”ç˜¡æœ€ç›¸è¿‘çš„è©

|word|similarity|
---|---
'è‚›è£‚'|0.8230267763137817
'å†…ç—”'|0.8147527575492859
'å¤–ç—”'|0.7933623194694519
'è„±è‚›'|0.7309954166412354
'è‚›ç˜˜'|0.7272818684577942
'ç—”'|0.721671998500824
'æ··åˆ'|0.6459854245185852
'è‚›é—¨'|0.6354331374168396
'ç—£'|0.6216157674789429
'è‚›'|0.5715659260749817

åŸæœ¬çš„ information åˆ†å¸ƒ

![word weight 0](https://cdn.discordapp.com/attachments/747728438814703616/989412309376040980/2022-06-21_094952.png)

æ›´æ”¹éçš„åˆ†å¸ƒ

![word weight 1](https://cdn.discordapp.com/attachments/747728438814703616/989412013497270282/weight222341341.png)

# **Classification**

ç‚ºäº†èƒ½å¤ è§£æ±ºå‰é¢æ‰€èªªçš„å•é¡Œï¼ŒæŠŠæ•´å€‹æ¡†æ¶å¾ sklearn pipeline æ›æˆè¼ƒç†Ÿæ‚‰çš„ pytorchï¼Œæ›æˆä½¿ç”¨TextCNN

![text cnn](https://miro.medium.com/max/1400/1*51dkqMhE21qKtzkEwl5PqA.jpeg)

é€é 3 åˆ° 4 æ¬¡çš„ Convolution 1D layerï¼Œä¾åºåšå‡ºå·ç©

## **Weighing**

ç‚ºåŸæœ‰çš„ word vector åŠ ä¸Šæ¬Šé‡

> **Improving text classification with weighted word embeddings via a multi-channel TextCNN model**\
> BaoGuo,ChunxiaZhang,JunminLiu,XiaoyiMa\
> DOI: [https://doi.org/10.1016/j.neucom.2019.07.052](https://www.sciencedirect.com/science/article/abs/pii/S0925231219310276)

åœ¨é€™ç¯‡ä¸­æœ‰æåˆ°ï¼Œweight vector èˆ‡ word vector å¯ä»¥ç›´æ¥ç›¸ä¹˜

```math
I_{i}=W_{i}\odotE, where \odot denotes the element-wise multiplication of two matrices
```

![textcnn w weight](https://cdn.discordapp.com/attachments/747728438814703616/989415319598678067/unknown.png)

æœ€å¾Œè¼¸å‡ºæˆ class å¤§å°

# **Result**

train äº†å¹¾æ¬¡ï¼Œæœ€å¥½çš„æº–ç¢ºç‡ä¾ç„¶åªæœ‰ 83%

è©¦è‘—åœ¨ TextCNN è£¡é¢èª¿æ•´åƒæ•¸

- é™ä½ dropout
- å°‡æœ€å¾Œçš„ linear layer å¢åŠ ç‚ºå…©å±¤(200 -> 50 -> 2)
- æé«˜ converlution kernel number and kernel size

éƒ½æ²’æœ‰å¤ªå¤§çš„æ”¹è®Šï¼Œä¾ç„¶æ˜¯åœ¨ 80% è‡³ 83% å¾˜å¾Š

![training accuracy](https://cdn.discordapp.com/attachments/747728438814703616/989428392971632720/WB_Chart_2022_6_23_3_13_10.png)
![training loss](https://cdn.discordapp.com/attachments/747728438814703616/989428400018051102/WB_Chart_2022_6_23_3_13_23.png)

å…¶å¯¦é™¤äº†æ›æˆå…¶ä»–æ¨¡å‹ï¼Œæƒ³ä¸åˆ°é‚„èƒ½ç”¨ä»€éº¼æŠ€è¡“è®“ä»–è®Šå¾—æ›´å¥½

# **Discussion**

ä¹‹æ‰€ä»¥ä¸ç”¨ Huggingface Bert æ˜¯å› ç‚ºä»¥å‰è©¦éäº†ï¼Œå¸Œæœ›èƒ½åœ¨èª²å ‚å ±å‘Šåˆ©ç”¨ä¸åŒçš„æŠ€è¡“è©¦è©¦çœ‹ã€‚

é‚„æœ‰å¾ˆæ±è¥¿æ²’æœ‰å˜—è©¦ï¼ŒELMoã€å…¶ä»–ä¸åŒçš„ word spaceã€ä¸åŒçš„ feature selection algorithmã€ä¸åŒçš„ torch model ç­‰ç­‰ã€‚åŸæœ¬è¨ˆç•«è¦ç”¨ Latent semantic analysisï¼Œä½†æ²’çœ‹å¾ˆæ‡‚ï¼Œä¸‹æ¬¡å†æŒ‘æˆ°ã€‚

# **Reference**

[LÃ³pez, F. & Miller, S. (2020, September 18). Text Classification with CNNs in PyTorch. Towardsdatascience.](https://towardsdatascience.com/text-classification-with-cnns-in-pytorch-1113df31e79f)

[Chandra, A. (2018, November 15). Feature Selection in Text Classification. Towardsdatascience.](https://towardsdatascience.com/feature-selection-on-text-classification-1b86879f548e)

[Hong, S. (2016, March 28). Improved Feature Weight Algorithm and Its Application to Text Classification. Hindawi.](https://www.hindawi.com/journals/mpe/2016/7819626/)

[Guo, B., Zhang, C., Liu, J. & Ma, X. (2019, October 21). Improving text classification with weighted word embeddings via a multi-channel TextCNN model. Neurocomputing](https://doi.org/10.1016/j.neucom.2019.07.052)

[Zhu, L., Wang, G. & Zou, X. (2017). Improved information gain feature selection method for Chinese text classification based on word embedding. ICSCA '17: Proceedings of the 6th International Conference on Software and Computer Applications, , 72â€“76.](https://doi.org/10.1145/3056662.3056671)

[Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification.. arXiv:1408.5882 [cs.CL]](https://arxiv.org/abs/1408.5882)

[A. (2018, August 7). AliMorty/Text-Classification. Github.](https://github.com/AliMorty/Text-Classification)

[S. (2020, October 14). Shawn1993/cnn-text-classification-pytorch. Github.](https://github.com/Shawn1993/cnn-text-classification-pytorch)
